{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes a tfrecords file produced by \"ralpn-make-dataset.ipynb\" notebook and trains/tests a neural network for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import io\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Conv2DTranspose, Cropping2D, Concatenate, Dropout, Reshape\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to know how many classes we are working with!\n",
    "# in this case 5: background, LK, RK, AA, IVC\n",
    "numClasses = 5\n",
    "\n",
    "# first load the dataset and figure out how big it is...\n",
    "datasetFileName = 'ralpn_a.tfrecords'\n",
    "dataset = tf.data.TFRecordDataset(datasetFileName)\n",
    "dataset_working = tf.data.TFRecordDataset(datasetFileName)\n",
    "\n",
    "finalTestsetFilename = 'ralpn_a_001.tfrecords'\n",
    "finalTestset = tf.data.TFRecordDataset(finalTestsetFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of examples in this dataset\n",
    "# ideally we'd have already computed this and imported data as a tf.data.Dataset\n",
    "# but just count the examples for now\n",
    "numExamples = sum([1 for i in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((512, 512, 1), (320, 320, 5)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# the tfrecords file is just a bunch of serialized images and classification masks ('labels')\n",
    "# we need to convert these data back into images and make an actual dataset to be fed into the CNN\n",
    "# mostly from: https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/deep-learning/mnist-tfrecords-to-tensorflow.html\n",
    "# this is directly from the Kaggle notebook\n",
    "\n",
    "# function to turn a \n",
    "#def _bytes_feature(value):\n",
    "#  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "#  if isinstance(value, type(tf.constant(0))):\n",
    "#    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "#  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'seg_mask': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "# convert images to tensors\n",
    "def imageStringsToTensors(example):\n",
    "    image_tensor    = tf.image.rgb_to_grayscale(tf.image.decode_png(example['image'], dtype=tf.dtypes.uint8))\n",
    "    seg_mask_tensor = tf.image.rgb_to_grayscale(tf.image.decode_png(example['seg_mask'], dtype=tf.dtypes.uint8))\n",
    "    image_tensor    = tf.image.resize(image_tensor, (512, 512))/255.0\n",
    "    seg_mask_tensor = tf.image.resize(seg_mask_tensor, (320,320))\n",
    "    #seg_mask_tensor = tf.reshape(seg_mask_tensor,[-1])\n",
    "    seg_mask_tensor = tf.cast(seg_mask_tensor,np.uint8)\n",
    "    seg_mask_tensor = tf.one_hot(tf.squeeze(seg_mask_tensor),numClasses) # one-hot encoding; undo this with tf.argmax(tensor,axis=2)\n",
    "    return image_tensor, seg_mask_tensor\n",
    "\n",
    "# convert serialized dataset into a MapDataset with \"image\" and \"class\" features which are still byte strings\n",
    "parsed_dataset = dataset.map(_parse_image_function)\n",
    "parsed_dataset = parsed_dataset.map(imageStringsToTensors)\n",
    "print(parsed_dataset)\n",
    "\n",
    "# create another copy of the dataset to be used for viewing examples now\n",
    "# can't tee() datasets and it doesn't look like just making a copy works as would be expected\n",
    "view_dataset = dataset.map(_parse_image_function)\n",
    "view_dataset = view_dataset.map(imageStringsToTensors)\n",
    "\n",
    "# parse final testset\n",
    "finalTestsetParsed = finalTestset.map(_parse_image_function)\n",
    "finalTestsetParsed = finalTestsetParsed.map(imageStringsToTensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply a segmentation mask to an image\n",
    "def apply_mask_to_image(image_arr,seg_mask):\n",
    "    mask_colors = np.array([[0, 0, 0], [255,191,0], [0,255,0], [255,84,84], [84,84,255]])/255.0\n",
    "    \n",
    "    # convert mask from one-hot to a dense 2D array if necessary\n",
    "    seg_mask = seg_mask.squeeze()\n",
    "    #print('seg mask dims:',seg_mask.ndim)\n",
    "    if(seg_mask.ndim == 3):\n",
    "        mask_arr_dense = tf.argmax(seg_mask,axis=2).numpy()\n",
    "    else:\n",
    "        mask_arr_dense = seg_mask\n",
    "    \n",
    "    #seg_mask = tf.argmax(seg_mask,axis=1).numpy()\n",
    "    #mask_arr_dense = seg_mask.reshape((320,320))\n",
    "   \n",
    "    #print('mask_arr_dense shape:',np.shape(mask_arr_dense.shape))\n",
    "\n",
    "    # embed reduced segmentation mask in the center of a 512x512 array of zeros\n",
    "    mask_arr_dense = np.squeeze(mask_arr_dense)\n",
    "    mask_arr = np.zeros([512,512])\n",
    "    mask_arr[96:416,96:416] = mask_arr_dense  # remember! numpy indexing is startidx:(stopidx+1)... weird...\n",
    "    \n",
    "    # otherwise just use this...\n",
    "    # mask_arr = mask_arr_dense;\n",
    "    \n",
    "    # image_arr comes in as a 512x512 matrix of float32 (?) values on [0,1]\n",
    "    # we need to expand it to be a 512x512x3 RGB image of the same type and range\n",
    "    #image_arr = tf.squeeze(image_arr)  # will come in as shape (512,512,1); remove last dimension; opposite is tf.expand_dims()\n",
    "    image_arr = np.expand_dims(image_arr,2)\n",
    "    image_arr = np.tile(image_arr,(1,1,3))\n",
    "    \n",
    "    # convert from RGB to HSV  (note: this could be done in tensorflow with tf.image.rgb_to_hsv)\n",
    "    image_hsv_arr = colors.rgb_to_hsv(image_arr)\n",
    "    \n",
    "    # apply mask for each label class\n",
    "    for labelIdx in range(5):\n",
    "        \n",
    "        # get label_mask\n",
    "        label_mask = (mask_arr == labelIdx)\n",
    "        \n",
    "        # get color\n",
    "        this_color_hsv = colors.rgb_to_hsv(mask_colors[labelIdx,:])\n",
    "        \n",
    "        # update hue\n",
    "        image_hsv_arr[:,:,0] = image_hsv_arr[:,:,0] + np.multiply(this_color_hsv[0], label_mask)\n",
    "        \n",
    "        # update saturation\n",
    "        image_hsv_arr[:,:,1] = image_hsv_arr[:,:,1] + np.multiply(this_color_hsv[1], label_mask)\n",
    "\n",
    "    # convert back to RGB\n",
    "    new_image =  colors.hsv_to_rgb(image_hsv_arr)\n",
    "        \n",
    "    # return masked image\n",
    "    return new_image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add random shifts and zooms to data\n",
    "#ds_new = dataset_working.window(3)\n",
    "\n",
    "#for el in ds_new:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at an example image\n",
    "for example in view_dataset.take(1):\n",
    "    #print(example)\n",
    "    print('Image tensor shape: ', example[0].shape)\n",
    "    print('Segmentation mask tensor shape: ', example[1].shape)\n",
    "\n",
    "    # extract image array\n",
    "    image_arr = example[0].numpy().squeeze() #if necessary\n",
    "     \n",
    "    # extract label array, then produce masked image\n",
    "    label_arr = example[1].numpy()   \n",
    "    image_masked_arr = apply_mask_to_image(image_arr,label_arr)\n",
    "    \n",
    "    # some other methods of displaying images\n",
    "    # note: display.Image != Image()... first is actually IPython.display.Image() and second is PIL.Image()\n",
    "    # display.display(display.Image(data=tf.image.encode_png(image_arr).numpy(),width=200))   # easily resizable, but can't put images next to each other (automatic newline after each)\n",
    "    # display.display(Image.fromarray(image_arr))   #### EASIEST WAY TO DISPLAY AN IMAGE FROM A NUMPY ARRAY!!!, BUT CAN'T SET DISPLAY SIZE? #########\n",
    "    # Image.fromarray((image_masked_arr*255).astype(np.uint8),mode='RGB') # works for 512x512x3 RGB image, but can't resize?\n",
    "    # Image.fromarray((image_arr*255).astype(np.uint8),mode='L') # works for 512x512 gray image, but can't resize?\n",
    "\n",
    "    # display with matplotlib\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sp_axes = fig.subplots(1,2)\n",
    "    sp_axes[0].imshow(image_arr,cmap='gray',vmin=0,vmax=1)\n",
    "    sp_axes[0].get_xaxis().set_visible(False)\n",
    "    sp_axes[0].get_yaxis().set_visible(False)\n",
    "    sp_axes[0].set_title('Raw')\n",
    "    \n",
    "    sp_axes[1].imshow(image_masked_arr)\n",
    "    sp_axes[1].get_xaxis().set_visible(False)\n",
    "    sp_axes[1].get_yaxis().set_visible(False)\n",
    "    sp_axes[1].set_title('Masked')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test sets\n",
    "# https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets\n",
    "test_size = int(0.05 * numExamples)\n",
    "val_size = int(0 * numExamples)\n",
    "train_size = numExamples - (test_size + val_size)\n",
    "ds_full = parsed_dataset.shuffle(numExamples)\n",
    "ds_train = ds_full.take(train_size)\n",
    "ds_test = ds_full.take(test_size)\n",
    "ds_val = ds_full.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch and shuffle TRAINING data\n",
    "ds_train = ds_train.cache()   # cache dataset for more efficient shuffling and prevent reloading in each epoch\n",
    "ds_train = ds_train.shuffle(train_size)  # shuffle with buffer set to same size as dataset (ideal)\n",
    "ds_train = ds_train.batch(2)   # batch(N) -> arrange into batches of size N elements each\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE) # performance enhancement https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch but DO NOT SHUFFLE TEST and VALIDATION data\n",
    "# directly from https://www.tensorflow.org/datasets/keras_example\n",
    "ds_test = ds_test.batch(2)   # batch(N) -> arrange into batches of size N elements each\n",
    "ds_test = ds_test.cache()      # cache dataset to prevent reloading in each epoch\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)  # performance enhancement https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sizes\n",
    "#tf.data.experimental.cardinality(parsed_dataset)   # will return: tf.data.experimental.INFINITE_CARDINALITY\n",
    "#print('Dataset size:',sum(1 for expmple in ds_full))\n",
    "#train_size_true = sum(1 for expmple in ds_train)\n",
    "#test_size_true = sum(1 for expmple in ds_test)\n",
    "#val_size_true = sum(1 for expmple in ds_val)\n",
    "#train_size_true + test_size_true + val_size_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API from: https://github.com/lambdal/TensorFlow2-tutorial/tree/master/01-basic-image-classification\n",
    "# Mostly following U-Net structure from Ronneberger2015\n",
    "\n",
    "#out_1a = Input(shape=(572, 572,1),name='1A')\n",
    "out_1a = Input(shape=(512, 512,1),name='1A')\n",
    "out_1b = Conv2D(64, (3, 3), activation='relu',name='1B')(out_1a)\n",
    "out_1c = Conv2D(64, (3, 3), activation='relu',name='1C')(out_1b)\n",
    "\n",
    "out_2a = MaxPooling2D(pool_size=(2, 2),name='2A')(out_1c)\n",
    "out_2b = Conv2D(128, (3, 3), activation='relu',name='2B')(out_2a)\n",
    "out_2c = Conv2D(128, (3, 3), activation='relu',name='2C')(out_2b)\n",
    "out_2d = Conv2D(128, (3, 3), activation='relu',name='2D')(out_2c)\n",
    "\n",
    "out_3a = MaxPooling2D(pool_size=(2, 2),name='3A')(out_2d)\n",
    "out_3b = Conv2D(256, (3, 3), activation='relu',name='3B')(out_3a)\n",
    "out_3c = Conv2D(256, (3, 3), activation='relu',name='3C')(out_3b)\n",
    "\n",
    "out_4a = MaxPooling2D(pool_size=(2, 2),name='4A')(out_3c)\n",
    "out_4b = Conv2D(512, (3, 3), activation='relu',name='4B')(out_4a)\n",
    "#out_4c = Conv2D(512, (3, 3), activation='relu',name='4C')(out_4b)\n",
    "out_4c_do = Conv2D(512, (3, 3), activation='relu',name='4C')(out_4b)\n",
    "out_4c = Dropout(rate=0.5,name='Drop4')(out_4c_do,training=True)       # not entirely sure of dropout placement, following Ronneberger's code\n",
    "\n",
    "out_5a = MaxPooling2D(pool_size=(2, 2),name='5A')(out_4c)\n",
    "out_5b = Conv2D(1024, (3, 3), activation='relu',name='5B')(out_5a)\n",
    "#out_5c = Conv2D(1024, (3, 3), activation='relu',name='5C')(out_5b)\n",
    "out_5c_do = Conv2D(1024, (3, 3), activation='relu',name='5C')(out_5b)\n",
    "out_5c = Dropout(rate=0.5,name='Drop5')(out_5c_do,training=True)       # not entirely sure of dropout placement, following Ronneberger's code\n",
    "\n",
    "out_4d = Cropping2D(cropping=((4, 4), (4, 4)),name='4D')(out_4c)\n",
    "out_4e = Conv2DTranspose(512, (2, 2), strides=(2, 2), activation='relu',name='4E')(out_5c)  # up-convolution: http://bamos.github.io/2016/08/09/deep-completion/\n",
    "out_4f = Concatenate(name='4F')([out_4d, out_4e])\n",
    "out_4g = Conv2D(512, (3, 3), activation='relu',name='4G')(out_4f)\n",
    "out_4h = Conv2D(512, (3, 3), activation='relu',name='4H')(out_4g)\n",
    "\n",
    "out_3d = Cropping2D(cropping=((16, 16), (16, 16)),name='3D')(out_3c)\n",
    "out_3e = Conv2DTranspose(256, (2, 2), strides=(2, 2), activation='relu',name='3E')(out_4h)\n",
    "out_3f = Concatenate(name='3F')([out_3d, out_3e])\n",
    "out_3g = Conv2D(256, (3, 3), activation='relu',name='3G')(out_3f)\n",
    "out_3h = Conv2D(256, (3, 3), activation='relu',name='3H')(out_3g)\n",
    "\n",
    "out_2e = Cropping2D(cropping=((40, 40), (40, 40)),name='2E')(out_2d)\n",
    "out_2f = Conv2DTranspose(128, (2, 2), strides=(2, 2), activation='relu',name='2F')(out_3h)\n",
    "out_2g = Concatenate(name='2G')([out_2e, out_2f])\n",
    "out_2h = Conv2D(128, (3, 3), activation='relu',name='2H')(out_2g)\n",
    "out_2i = Conv2D(128, (3, 3), activation='relu',name='2I')(out_2h)\n",
    "out_2j = Conv2D(128, (3, 3), activation='relu',name='2J')(out_2i)\n",
    "\n",
    "#out_1d = Cropping2D(cropping=((88, 88), (88, 88)),name='1D')(out_1c)\n",
    "out_1d = Cropping2D(cropping=((92, 92), (92, 92)),name='1D')(out_1c)\n",
    "out_1e = Conv2DTranspose(64, (2, 2), strides=(2, 2), activation='relu',name='1E')(out_2j)\n",
    "out_1f = Concatenate(name='1F')([out_1d, out_1e])\n",
    "out_1g = Conv2D(64, (3, 3), activation='relu',name='1G')(out_1f)\n",
    "out_1h = Conv2D(64, (3, 3), activation='relu',name='1H')(out_1g)\n",
    "out_1i = Conv2D(numClasses, (1, 1), activation='softmax',name='1I')(out_1h)\n",
    "#out_classify = Reshape((102400,1))(out_1i)\n",
    "#x = Flatten()(out_classify)\n",
    "#x = Dense(5, activation='softmax')(out_classify)\n",
    "model = Model(inputs=out_1a, outputs=out_1i)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True,to_file='model_plot.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    #weights = tf.constant(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /=tf.keras.backend.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = tf.keras.backend.clip(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        # calc\n",
    "        loss = y_true *tf.keras.backend.log(y_pred) * weights\n",
    "        loss = -tf.keras.backend.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight losses due to class imbalence\n",
    "class_weight_dic = {0: 0.01,\n",
    "                    1: 1.0,\n",
    "                    2: 1.0,\n",
    "                    3: 1.0,\n",
    "                    4: 1.0}\n",
    "\n",
    "weights = tf.constant([0.0016,0.1545,0.1442,0.3812,0.3185],dtype=tf.float32)\n",
    "\n",
    "#model.compile(optimizer='sgd',\n",
    "#              loss='mse',\n",
    "#              metrics=['accuracy',tf.keras.metrics.MeanIoU(num_classes=5)])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=weighted_categorical_crossentropy(weights),\n",
    "              metrics=['accuracy',tf.keras.metrics.MeanIoU(num_classes=5)])\n",
    "\n",
    "\n",
    "tfa.losses.GIoULoss()\n",
    "#model.compile(optimizer='adam',\n",
    "#              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#              metrics=['accuracy',tf.keras.metrics.MeanIoU(num_classes=5)])\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train model\n",
    "model_history = model.fit(ds_train,epochs=20,validation_data=ds_test)#,class_weight=class_weight_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at some predictions\n",
    "for example in ds_test.take(10):\n",
    "    #print(example)\n",
    "    #print('Image tensor shape: ', example[0].shape)\n",
    "    #print('Segmentation mask tensor shape: ', example[1].shape)\n",
    "\n",
    "    # extract image array\n",
    "    image_arr = example[0].numpy()\n",
    "    image_arr = image_arr[0,:,:,:].squeeze()\n",
    "\n",
    "    #print('image_arr shape:', np.shape(image_arr))\n",
    "    \n",
    "    \n",
    "    # extract label array, then produce masked image\n",
    "    \n",
    "    seg_mask_arr = example[1].numpy()\n",
    "    seg_mask_arr = seg_mask_arr[0,:,:,:].squeeze()\n",
    "    #print('seg_mask_arr shape:', np.shape(seg_mask_arr))\n",
    "    image_masked_arr = apply_mask_to_image(image_arr,seg_mask_arr)\n",
    "    #print('image_masked_arr shape:', np.shape(image_masked_arr))\n",
    "   \n",
    "    # make a prediction\n",
    "    predict_seg_mask_arr = model.predict(example[0])\n",
    "    predict_seg_mask_arr = predict_seg_mask_arr[0,:,:,:].squeeze()\n",
    "    #print('predict_seg_mask_arr shape:', np.shape(predict_seg_mask_arr))\n",
    "    predict_image_masked_arr = apply_mask_to_image(image_arr,predict_seg_mask_arr)\n",
    "    #print('predict_image_masked_arr shape:', np.shape(predict_image_masked_arr))\n",
    "    \n",
    "    # some other methods of displaying images\n",
    "    # note: display.Image != Image()... first is actually IPython.display.Image() and second is PIL.Image()\n",
    "    # display.display(display.Image(data=tf.image.encode_png(image_arr).numpy(),width=200))   # easily resizable, but can't put images next to each other (automatic newline after each)\n",
    "    # display.display(Image.fromarray(image_arr))   #### EASIEST WAY TO DISPLAY AN IMAGE FROM A NUMPY ARRAY!!!, BUT CAN'T SET DISPLAY SIZE? #########\n",
    "    # Image.fromarray((image_masked_arr*255).astype(np.uint8),mode='RGB') # works for 512x512x3 RGB image, but can't resize?\n",
    "    # Image.fromarray((image_arr*255).astype(np.uint8),mode='L') # works for 512x512 gray image, but can't resize?\n",
    "\n",
    "    # display with matplotlib\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sp_axes = fig.subplots(1,3)\n",
    "    sp_axes[0].imshow(image_arr,cmap='gray',vmin=0,vmax=1)\n",
    "    sp_axes[0].get_xaxis().set_visible(False)\n",
    "    sp_axes[0].get_yaxis().set_visible(False)\n",
    "    sp_axes[0].set_title('Raw')\n",
    "    \n",
    "    sp_axes[1].imshow(image_masked_arr)\n",
    "    sp_axes[1].get_xaxis().set_visible(False)\n",
    "    sp_axes[1].get_yaxis().set_visible(False)\n",
    "    sp_axes[1].set_title('Masked (Truth)')\n",
    "    \n",
    "    sp_axes[2].imshow(predict_image_masked_arr)\n",
    "    sp_axes[2].get_xaxis().set_visible(False)\n",
    "    sp_axes[2].get_yaxis().set_visible(False)\n",
    "    sp_axes[2].set_title('Masked (Prediction)')\n",
    "    \n",
    "    #seg_mask_out = tf.argmax(predict_seg_mask_arr,axis=2).numpy()\n",
    "    \n",
    "    #print(seg_mask_out)\n",
    "    #print('max: ',np.amax(predict_seg_mask_arr,(0,1,2)))\n",
    "    sio.savemat('test.mat', {'predict_seg_mask_arr':predict_seg_mask_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some predictions\n",
    "for example in finalTestsetParsed.take(2):\n",
    "    #print(example)\n",
    "    #print('Image tensor shape: ', example[0].shape)\n",
    "    #print('Segmentation mask tensor shape: ', example[1].shape)\n",
    "\n",
    "    # extract image array\n",
    "    image_arr = example[0].numpy()\n",
    "    #image_arr = image_arr[0,:,:].squeeze()\n",
    "\n",
    "    #print('image_arr shape:', np.shape(image_arr))\n",
    "    \n",
    "    \n",
    "    # extract label array, then produce masked image\n",
    "    \n",
    "    seg_mask_arr = example[1].numpy()\n",
    "    #seg_mask_arr = seg_mask_arr[0,:,:,:].squeeze()\n",
    "    #print('seg_mask_arr shape:', np.shape(seg_mask_arr))\n",
    "    image_masked_arr = apply_mask_to_image(image_arr,seg_mask_arr)\n",
    "    #print('image_masked_arr shape:', np.shape(image_masked_arr))\n",
    "   \n",
    "    # make a prediction\n",
    "    predict_seg_mask_arr = model.predict(example[0])\n",
    "    predict_seg_mask_arr = predict_seg_mask_arr[0,:,:,:].squeeze()\n",
    "    #print('predict_seg_mask_arr shape:', np.shape(predict_seg_mask_arr))\n",
    "    predict_image_masked_arr = apply_mask_to_image(image_arr,predict_seg_mask_arr)\n",
    "    #print('predict_image_masked_arr shape:', np.shape(predict_image_masked_arr))\n",
    "    \n",
    "    # some other methods of displaying images\n",
    "    # note: display.Image != Image()... first is actually IPython.display.Image() and second is PIL.Image()\n",
    "    # display.display(display.Image(data=tf.image.encode_png(image_arr).numpy(),width=200))   # easily resizable, but can't put images next to each other (automatic newline after each)\n",
    "    # display.display(Image.fromarray(image_arr))   #### EASIEST WAY TO DISPLAY AN IMAGE FROM A NUMPY ARRAY!!!, BUT CAN'T SET DISPLAY SIZE? #########\n",
    "    # Image.fromarray((image_masked_arr*255).astype(np.uint8),mode='RGB') # works for 512x512x3 RGB image, but can't resize?\n",
    "    # Image.fromarray((image_arr*255).astype(np.uint8),mode='L') # works for 512x512 gray image, but can't resize?\n",
    "\n",
    "    # display with matplotlib\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    sp_axes = fig.subplots(1,3)\n",
    "    sp_axes[0].imshow(image_arr,cmap='gray',vmin=0,vmax=1)\n",
    "    sp_axes[0].get_xaxis().set_visible(False)\n",
    "    sp_axes[0].get_yaxis().set_visible(False)\n",
    "    sp_axes[0].set_title('Raw')\n",
    "    \n",
    "    sp_axes[1].imshow(image_masked_arr)\n",
    "    sp_axes[1].get_xaxis().set_visible(False)\n",
    "    sp_axes[1].get_yaxis().set_visible(False)\n",
    "    sp_axes[1].set_title('Masked (Truth)')\n",
    "    \n",
    "    sp_axes[2].imshow(predict_image_masked_arr)\n",
    "    sp_axes[2].get_xaxis().set_visible(False)\n",
    "    sp_axes[2].get_yaxis().set_visible(False)\n",
    "    sp_axes[2].set_title('Masked (Prediction)')\n",
    "    \n",
    "    #seg_mask_out = tf.argmax(predict_seg_mask_arr,axis=2).numpy()\n",
    "    \n",
    "    #print(seg_mask_out)\n",
    "    #print('max: ',np.amax(predict_seg_mask_arr,(0,1,2)))\n",
    "    sio.savemat('test.mat', {'predict_seg_mask_arr':predict_seg_mask_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arr = example[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a bunch of stuff\n",
    "myarr = np.array([[1,2],[3,4]])\n",
    "print(myarr == 2)\n",
    "myarr2 = np.multiply(myarr,(myarr==2))\n",
    "print(myarr2)\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "myarr3d = np.array([ [[1,1],[1,1]], [[2,2],[2,2]], [[3,3],[3,3]] ])\n",
    "print(myarr3d)\n",
    "print(myarr3d.shape)\n",
    "myarr3d_slice = myarr3d[0,:,:]\n",
    "\n",
    "print(myarr3d_slice)\n",
    "\n",
    "mycolors = np.array([[0, 0, 0], [255,191,255], [0,255,0], [255,84,84], [84,84,255]])\n",
    "print(mycolors[1,:])\n",
    "print(colors.rgb_to_hsv(mycolors[1,:]))\n",
    "\n",
    "myarr2d = myarr3d[0,:,:] + myarr3d[1,:,:]\n",
    "print(myarr2d)\n",
    "\n",
    "np.ones((4,4))\n",
    "np.ones(myarr2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myarr = np.array([[1,2],[3,4]])\n",
    "newArr = np.expand_dims(myarr,2)\n",
    "newArr = np.tile(newArr,(1,1,3))\n",
    "print('Shape:', np.shape(newArr),'\\n')\n",
    "print(newArr[:,:,0],'\\n')\n",
    "print(newArr[:,:,1],'\\n')\n",
    "print(newArr[:,:,2],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myarr2d = np.array([[1,2],[3,4]])\n",
    "print(myarr2d)\n",
    "myarr2d[0,:] = myarr2d[0,:] + np.ones(myarr2d[0,:].shape)\n",
    "print(myarr2d)\n",
    "\n",
    "red = np.array([255,0,0])\n",
    "print(red)\n",
    "red_hsv = colors.rgb_to_hsv(red/255.0)\n",
    "print(red_hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate one-hot encoding using tf.one_hot(), and returning to dense representation using tf.argmax()\n",
    "numClasses = 5\n",
    "labelTensor = tf.constant([[1, 2],[0,1]])\n",
    "print('Label tensor:',labelTensor,'\\n')\n",
    "oh = tf.one_hot(labelTensor,numClasses)\n",
    "print('One hot:', oh,'\\n')\n",
    "dense = tf.argmax(oh.numpy(),axis=2)  # axis = 2 if we have a 2D mask (mask dims are axes 0 and 1; axis #2 is the one hot encoding dimension)\n",
    "print('Dense:', dense,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallMask = np.array([[1,2],[3,4]])\n",
    "print(smallMask,'\\n')\n",
    "largeMask = np.zeros((10,10))\n",
    "print(largeMask,'\\n')\n",
    "largeMask[4:6,4:6] = smallMask  # remember! numpy indexing is startidx:(stopidx+1)... weird...\n",
    "print(largeMask,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
